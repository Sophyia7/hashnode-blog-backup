---
title: "How to Set Up Ollama and Run LLMs Locally"
seoTitle: "Set Up Ollama and Run LLMs Locally"
seoDescription: "Learn how to set up Ollama and run large language models locally on your computer with this step-by-step guide"
datePublished: Mon Sep 22 2025 12:19:47 GMT+0000 (Coordinated Universal Time)
cuid: cmfv3g404000102jvd0a88tib
slug: how-to-set-up-ollama-and-run-llms-locally
canonical: https://apptaliclab.com/blog/set-up-ollama-run-llms-locally/
ogImage: https://cdn.hashnode.com/res/hashnode/image/upload/v1758542865334/d9c62460-33ab-474d-bdfa-13bd1fccf4ce.png
tags: ai, python, llm, ollama

---

Most people hear the term “AI model” and think of something that exists only in the cloud, accessed through an API key, and has a monthly fee. But not all large language models (LLMs) have to live on a cloud, a farm data center. Ollama lets you and me have LLMs on our computers. This gives us the flexibility to have the LLM regardless of internet connection, $0 in the bank, and our chats can be saved on our computers as well.

In short, Ollama is a local LLM runtime; it’s a lightweight environment that lets you download, run, and chat with LLMs locally; It’s like VSCode for LLMs. Although if you want to run an LLM on a container (like Docker), that is also an option. The goal of Ollama is to handle the heavy lifting of executing models and managing memory, so you can focus on using the model rather than wiring it from scratch.

In this short guide, we will walk through the steps of installing ollama, downloading models, and building with the model in a simple Python project.

## **Prerequisites**

To follow alongside this guide, you will need to:

* Have a laptop or PC that has 16GB RAM minimum
    
* Windows 10+ or MacOS 12+
    

## **Installing** [**Ollama**](https://ollama.com/)

Before installing Ollama, ensure you have met the prerequisites above for a smooth experience using Ollama.

Head over to their website, download the software, and click on Download. You will be redirected to the download page, where you can select an OS and follow the on-screen instructions to install it on your computer.

![](https://apptaliclab.com/wp-content/uploads/2025/09/Screenshot-2025-09-21-at-7.56.32-PM-1024x482.png align="left")

To confirm you have Ollama installed properly, you can run this command:

```bash
ollama --version
```

You should see the version of Ollama you have installed

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1758542241917/e153ce0e-ed03-4440-8c1b-4f33a6f19a71.png align="center")

## **Downloading a Model**

Once you have the Ollama software installed, you will need to download a model. Ollama has an awesome list of open-source models to choose from; they also offer embedding models. Head over to their [model](https://ollama.com/search) section and pick a model. For this guide, we will download the [`deepseek-r1`](https://ollama.com/library/deepseek-r1) model.

Something to keep in mind when downloading a model is that each model has different versions that have different parameter sizes. A parameter is a representation of data that the model was trained on or the data it has learned. Larger parameter sizes mean better response, especially if you want it for complex tasks.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1758542228368/c66d4b57-e5a0-4024-a0a9-f586f39b5c4b.png align="center")

We will download the lowest parameter size, so click on the size you want to download and copy the command from the page onto your terminal.

```bash
ollama run deepseek-r1:1.5b
```

To confirm you have downloaded the model, run the command again, and it should let you send a message to the model.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1758542183128/576fcfa0-9928-41ca-af2d-cf95d7d8221a.png align="center")

This lets you run the model you just downloaded locally, and with this, you can send messages to the model locally.

This is cool, but it doesn’t stop here. Let’s see how this model works in a project like a simple chatbot.

## Accessing Ollama Endpoint

Ollama has an endpoint that lets you call the downloaded model and interact. This section will feature a simple project built in Python, which is optional, as the endpoint works in any project as long as you have the necessary software downloaded.

You would need to download `requests` library using pip to run the project, do this inside your virtual environment.

```bash
pip install requests
```

We would need import requests and JSON.

```python
import requests
import json
```

Then you need to call the Ollama localhost endpoint and define the model name. This endpoint lets you send requests to models you downloaded and get a response.

```python
OLLAMA_URL = "<http://localhost:11434/api/generate>"
MODEL = "deepseek-r1:1.5b"
```

Next, define functions that would take user input and send it to the model via ollama.

The `ask_ollama` func takes the user input as a prompt as an argument and sends it as a POST request to the model.

```python
def ask_ollama(prompt):
    payload = {
        "model": MODEL,
        "prompt": prompt,
        "stream": False 
    }
    try:
        response = requests.post(OLLAMA_URL, json=payload)
        response.raise_for_status()
        
        result = response.json()
        if "response" in result:
            return result["response"]
        else:
            return "No response received from the model"
            
    except requests.exceptions.ConnectionError:
        return "Error: Cannot connect to Ollama. Make sure it's running (ollama serve)"
    except requests.exceptions.HTTPError as he:
        if "404" in str(he):
            return f"Error: Model '{MODEL}' not found. Try running: ollama pull {MODEL}"
        return f"HTTP Error: {he}"
    except json.JSONDecodeError:
        return "Error: Invalid response from Ollama"
    except Exception as e:
        return f"Error: {e}
```

Then, the `main` func is the main function of the project, which takes the user's input and returns the response from the model. It also tells the user the model sending the request.

```python
 def main():
    print(f"Chatbot using {MODEL} via Ollama. Type 'exit' to quit.")
    while True:
        user_input = input("You: ")
        if user_input.lower() == "exit":
            break
        print("Bot:", ask_ollama(user_input).strip())

if __name__ == "__main__":
    main()
```

Your output should look like this.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1758542166991/10c0b35c-4c43-42a5-9c07-74b817c6430e.png align="center")

## Wrapping Up

And that is how you can interact with models you download. This lets you have open-source models you can use to experiment with building AI-powered solutions or agents.

The downside of using a local model rather than a cloud-based model is that it may not always provide the desired results, as they are often trained on a small dataset or lack real-time data, i.e., outdated datasets. Regardless, this is a great way to save costs on interacting with GPT, Claude, etc, for a simple project.

If you prefer the video version of this guide, you should check out the video below

%[https://youtu.be/K6lQ9RW5gEc?si=tvBjg5JNkxthB62Q]